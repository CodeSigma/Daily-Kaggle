{"cells":[{"metadata":{},"cell_type":"markdown","source":"Today I'll try to predict wether the passenger survived or not during the Titanic Disaster"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert NaN into 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(0)\ntest = test.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm just want to inspect how many people survived from the disaster in the train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the cabin code means something.\n\nI think the first letter means something like the cabin position, so I'll group them by the first letter"},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_type = []\ncab = train[\"Cabin\"]\nfor x in cab:\n    if str(x)[0:1] not in cabin_type:\n        cabin_type.append(str(x)[0:1])\n        \n        \ncabin_type\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also interested in Embarked colum"},{"metadata":{"trusted":true},"cell_type":"code","source":"embarked = []\ncol = train[\"Embarked\"]\nfor x in col:\n    if str(x)[0:1] not in embarked:\n        embarked.append(str(x)[0:1])\n        \n        \nembarked\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I still not able to use the string feature, so I'm gonna convert them to integer that represents their class"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain[\"Cabin\"] = [cabin_type.index(str(x)[0:1]) for x in train[\"Cabin\"]]\ntest[\"Cabin\"] = [cabin_type.index(str(x)[0:1]) for x in test[\"Cabin\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's just a few people in the Cabin, 0 means NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train[\"Cabin\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Sex\"] = [int(x == \"male\") for x in train[\"Sex\"]]\ntest[\"Sex\"] = [int(x == \"male\") for x in test[\"Sex\"]]\n#1 = male","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I know that most of the Titanic survivor is Female, I'm gonna ensure that"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train[\"Sex\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train[train[\"Survived\"] == 1][\"Sex\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of Titanic Passengers are male but the survivors are mostly female.\n(Ladies first, hats off)"},{"metadata":{},"cell_type":"markdown","source":"Now I'm inspecting the Embarked column. I'm not sure what it is but I think that's the place where the Passenger get into the ship"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Embarked\"] = [embarked.index(str(x)) for x in train[\"Embarked\"]]\ntest[\"Embarked\"] = [embarked.index(str(x)) for x in test[\"Embarked\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train[\"Embarked\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's the dataframe after I changed the string features, I guess the Name and Ticket won't do anything so I'll drop it later"},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect the correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(train.corr(), annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()[\"Survived\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop the features with less correlation to the Survived column"},{"metadata":{"trusted":true},"cell_type":"code","source":"passenger_id = test[\"PassengerId\"]\ntrain = train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Age\", \"SibSp\", \"Parch\"], axis=1)\ntest = test.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Age\", \"SibSp\", \"Parch\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop([\"Survived\"], axis=1)\ny_train = train[\"Survived\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the test dataframe has no label, so let's put all of the columns to X_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Let's try use the Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nlogr.fit(X_train, y_train)\nlogr.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = logr.predict(X_test)\npred = [int(x >= 0.5) for x in pred]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This cell is to make the submission spreadsheet"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'PassengerId': passenger_id, 'Survived': pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Neural Network"},{"metadata":{},"cell_type":"markdown","source":"I'm also trying to use Deep Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import *\n# from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\nfrom keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nmodel = Sequential()\nmodel.add(Dense(32))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(1))\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.compile(optimizer = Adam(1e-4),\n             loss = \"binary_crossentropy\",\n             metrics = [\"accuracy\", f1_m])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(patience=20, verbose=1, monitor='val_f1_m', mode='max')\nreduce_lr =  ReduceLROnPlateau(monitor='val_f1_m', factor=0.1, patience=20, min_delta=1e-4, mode='max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, \n          y_train, \n          validation_split = 0.3,\n          callbacks=[early_stop, reduce_lr],\n          epochs = 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_nn = model.predict(X_test)\npred_nn = [int(x >= 0.5) for x in pred_nn]\n# model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This also for the submission spreadsheet"},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_sub = pd.DataFrame({'PassengerId': passenger_id, 'Survived': pred_nn})\n# you could use any filename. We choose submission here\nNN_sub.to_csv('submission_NN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Today I'm gonna try Random Forest and Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=2)\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nrf.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_sub = pd.DataFrame({'PassengerId': passenger_id, 'Survived': rf_pred})\n# you could use any filename. We choose submission here\nrf_sub.to_csv('submission_RF.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this model scored 0.779"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(learning_rate=0.06, n_estimators = 100,validation_fraction = 0.2, n_iter_no_change=100)\ngb.fit(X_train, y_train)\ngb.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_pred =  gb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_sub = pd.DataFrame({'PassengerId': passenger_id, 'Survived': gb_pred})\n# you could use any filename. We choose submission here\ngb_sub.to_csv('submission_GB.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this model scored 0.775"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"My Logistic Regression model is only scored around 0.75, and my Neural Network model is only scored around 0.73\n\nToday I tried improving my Neuran Network model but it scored less, I also tried Random Forest and Gradient Boosting, I got 0.779 from Random Forest and 0.775 from Gradient Boosting\n\nI was too naive to think that I can surpass 0.85 in the first or second try.\n\nI guess I'll try another dataset tomorrow, I have to gather more experiences and skills, then improve these models again later..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}